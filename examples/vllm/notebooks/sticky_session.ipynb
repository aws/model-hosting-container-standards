{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc3ad8f",
   "metadata": {},
   "source": [
    "# Sticky Session Routing\n",
    "\n",
    "This notebook demonstrates how to use sticky session routing with vLLM on SageMaker.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Sticky session routing enables all requests from the same session to be routed to the same model instance, allowing your application to reuse previously processed information.\n",
    "\n",
    "### How It Works\n",
    "1. Enable sessions with `SAGEMAKER_ENABLE_STATEFUL_SESSIONS=true`\n",
    "2. Create a session with `SessionId=\"NEW_SESSION\"`\n",
    "3. Use the returned session ID for subsequent requests\n",
    "4. Close the session when done to free resources\n",
    "\n",
    "## Prerequisites\n",
    "- SageMaker execution role with appropriate permissions\n",
    "- vLLM container image in ECR\n",
    "- HuggingFace token for gated models (e.g., Llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031960ea-87c6-4d01-8a78-1860e6e9079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e2532-a8c1-4e87-b629-ad3c3242b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "runtime_client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "sts_client = boto3.client('sts', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901518a3-e7df-4853-a739-d8e0dfebf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "model_name = f'vllm-sticky-session-{timestamp}'\n",
    "endpoint_config_name = f'vllm-sticky-session-config-{timestamp}'\n",
    "endpoint_name = f'vllm-sticky-session-{timestamp}'\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7647e7-7ed2-4201-acd3-8c21726a0764",
   "metadata": {},
   "source": [
    "## 1. Create Model with Sticky Sessions Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c24c4-c8c5-430c-b4a0-ead2f27de6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image = f'{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:0.11.2-sagemaker-v1.2'\n",
    "huggingface_model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "huggingface_token = ''  # Replace with your actual token\n",
    "instance_type = 'ml.g6.4xlarge'\n",
    "execution_role = f'arn:aws:iam::{account_id}:role/SageMakerExecutionRole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768ea56-095b-4711-af46-c6de766a9a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model Name: vllm-sticky-session-20251211-225002\n",
      "  Endpoint Name: vllm-sticky-session-20251211-225002\n",
      "  HuggingFace Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  Instance Type: ml.g6.4xlarge\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuration:\")\n",
    "print(f\"  Model Name: {model_name}\")\n",
    "print(f\"  Endpoint Name: {endpoint_name}\")\n",
    "print(f\"  HuggingFace Model: {huggingface_model_id}\")\n",
    "print(f\"  Instance Type: {instance_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad0576-b730-4ac2-b32a-f25ecf546137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model created\n",
      "  Model ARN: arn:aws:sagemaker:us-west-2:875423407011:model/vllm-sticky-session-20251211-225002\n"
     ]
    }
   ],
   "source": [
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container_image,\n",
    "        'Environment': {\n",
    "            'SM_VLLM_MODEL': huggingface_model_id,\n",
    "            'HUGGING_FACE_HUB_TOKEN': huggingface_token,\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': 'INFO',\n",
    "            'SM_VLLM_MAX_MODEL_LEN': '2048',\n",
    "            # Enable sticky session routing\n",
    "            'SAGEMAKER_ENABLE_STATEFUL_SESSIONS': 'true',\n",
    "        }\n",
    "    },\n",
    "    ExecutionRoleArn=execution_role,\n",
    ")\n",
    "print(f\"âœ“ Model created\")\n",
    "print(f\"  Model ARN: {create_model_response['ModelArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36312f41-e33f-4e59-afcf-4533db10fadb",
   "metadata": {},
   "source": [
    "## 2. Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72154799-0de1-4226-942c-8ba8e1d4c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nCreating endpoint configuration: {endpoint_config_name}\")\n",
    "\n",
    "# Note: Use 2+ instances to see sticky routing in action\n",
    "# With 1 instance, all requests go to the same instance anyway\n",
    "instance_count = 2\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': model_name,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialInstanceCount': instance_count,\n",
    "            'InitialVariantWeight': 1.0,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Endpoint configuration created\")\n",
    "print(f\"  Config ARN: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad58596-5b95-41c8-9e2d-1cde5f9aa5a5",
   "metadata": {},
   "source": [
    "## 3. Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a99e8-119e-47b3-a784-29b7e51dab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating endpoint: vllm-sticky-session-20251211-225002\n",
      "â±ï¸  This will take approximately 5-10 minutes...\n",
      "\n",
      "ðŸ’¡ Monitor progress: https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints/vllm-sticky-session-20251211-225002\n",
      "\n",
      "âœ“ Endpoint creation initiated\n",
      "  Endpoint ARN: arn:aws:sagemaker:us-west-2:875423407011:endpoint/vllm-sticky-session-20251211-225002\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating endpoint: {endpoint_name}\")\n",
    "print(\"â±ï¸  This will take approximately 5-10 minutes...\")\n",
    "print(f\"\\nðŸ’¡ Monitor progress: https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\\n\")\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Endpoint creation initiated\")\n",
    "print(f\"  Endpoint ARN: {create_endpoint_response['EndpointArn']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90486461-3ce0-4e6f-a306-c433cccbbb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waiting for endpoint to be in service...\n",
      "(This may take 5-10 minutes - please be patient)\n",
      "\n",
      "âœ“ Endpoint is now in service!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWaiting for endpoint to be in service...\")\n",
    "print(\"(This may take 5-10 minutes - please be patient)\\n\")\n",
    "\n",
    "waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={\n",
    "        'Delay': 20,\n",
    "        'MaxAttempts': 60\n",
    "    }\n",
    ")\n",
    "print(\"âœ“ Endpoint is now in service!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb8178-2bc9-4182-b414-f0cbc6952df5",
   "metadata": {},
   "source": [
    "## 4. Create a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9414d56-c68d-466d-8739-9c8dc1095531",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    SessionId=\"NEW_SESSION\",\n",
    "    Body=json.dumps({\"requestType\": \"NEW_SESSION\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d1ef2-8bd7-4082-91e5-c99e0d77a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_session_id_from_header(header_value: str) -> str:\n",
    "    \"\"\"Extract session ID from SageMaker session header.\n",
    "\n",
    "    Header format: \"<uuid>; Expires=<timestamp>\"\n",
    "    \"\"\"\n",
    "    # The session ID is before the semicolon\n",
    "    if \";\" in header_value:\n",
    "        return header_value.split(\";\")[0].strip()\n",
    "    return header_value.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3feca60-6fe0-4a0a-b4fb-48bdf1101b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get session ID from response header\n",
    "header_value = response['ResponseMetadata']['HTTPHeaders']['x-amzn-sagemaker-new-session-id']\n",
    "session_id = extract_session_id_from_header(header_value)\n",
    "print(f\"âœ“ Session created\")\n",
    "print(f\"  Session ID: {session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2bd4f6-8419-456f-b346-0d8e85e636a9",
   "metadata": {},
   "source": [
    "## 5. Use the Session (Chat Completions)\n",
    " \n",
    "Using OpenAI-compatible chat completions format. With sticky sessions,\n",
    "the KV cache from previous turns can be reused for faster inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08125d98-92d3-42a5-ba26-8f81d6a8b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "messages = []\n",
    "\n",
    "# First message - introduce yourself\n",
    "messages.append({\"role\": \"user\", \"content\": \"Hi! My name is Alice and I love hiking.\"})\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    SessionId=session_id,\n",
    "    Body=json.dumps({\n",
    "        \"model\": huggingface_model_id,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd935f-a390-485f-bb9a-851033ba986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi! My name is Alice and I love hiking.\n",
      "Assistant: Hi Alice! It's great to meet you! I'm glad to hear that you love hiking. There's something about being in nature, surrounded by trees, mountains, and wildlife, that can be so uplifting and rejuvenating. What kind of hiking do you enjoy the most? Do you have a favorite trail or location that you like to hike?\n",
      "\n",
      "Usage: {'prompt_tokens': 21, 'total_tokens': 92, 'completion_tokens': 71, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = json.loads(response['Body'].read())\n",
    "assistant_message = result['choices'][0]['message']['content']\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "print(\"User: Hi! My name is Alice and I love hiking.\")\n",
    "print(f\"Assistant: {assistant_message}\")\n",
    "print(f\"\\nUsage: {result.get('usage', {})}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71f15d-48a5-4665-9f9c-5df5472339dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's my name and what do I enjoy doing?\n",
      "Assistant: Your name is Alice, and you enjoy hiking!\n",
      "\n",
      "Usage: {'prompt_tokens': 112, 'total_tokens': 123, 'completion_tokens': 11, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "# Second message - ask about previous context (tests KV cache reuse)\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's my name and what do I enjoy doing?\"})\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    SessionId=session_id,\n",
    "    Body=json.dumps({\n",
    "        \"model\": huggingface_model_id,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    })\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read())\n",
    "assistant_message = result['choices'][0]['message']['content']\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "print(\"User: What's my name and what do I enjoy doing?\")\n",
    "print(f\"Assistant: {assistant_message}\")\n",
    "print(f\"\\nUsage: {result.get('usage', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b43ae-a4f6-46da-b0b5-31acc096f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can you suggest a good hiking trail?\n",
      "Assistant: I'd be happy to suggest a good hiking trail!\n",
      "\n",
      "Since you didn't specify a location or a level of difficulty, I'll suggest a few popular trails that are suitable for most hikers:\n",
      "\n",
      "1. **Appalachian Trail, USA**: The Appalachian Trail is a 2,190-mile long trail that spans from Georgia to Maine. It's a challenging trail, but it's also incredibly scenic and offers stunning views of the Appalachian Mountains.\n",
      "2. **Inca Trail, Peru**: The Inca Trail is a 26-mile long trail that leads to Machu Picchu, one of the Seven Wonders of the World. It's a challenging hike, but the scenery is breathtaking, and the history is rich.\n",
      "3. **Troll\n",
      "\n",
      "Usage: {'prompt_tokens': 140, 'total_tokens': 290, 'completion_tokens': 150, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "# Third message - continue the conversation\n",
    "messages.append({\"role\": \"user\", \"content\": \"Can you suggest a good hiking trail?\"})\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    SessionId=session_id,\n",
    "    Body=json.dumps({\n",
    "        \"model\": huggingface_model_id,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    })\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read())\n",
    "assistant_message = result['choices'][0]['message']['content']\n",
    "\n",
    "print(\"User: Can you suggest a good hiking trail?\")\n",
    "print(f\"Assistant: {assistant_message}\")\n",
    "print(f\"\\nUsage: {result.get('usage', {})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060245f-fecc-4a2b-9625-37b20582ffc2",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "In the cloudwatch log, We can see that the session is created and all conversation is routed to the same instance. The Prefix cache is also utilized for the same conversation.\n",
    "\n",
    "```\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m [INFO] model_hosting_container_standards - handlers.py:80: Session 99680e4c-5d0e-49ac-88e7-103866569520 created\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"POST /invocations HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:37650 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO 12-11 16:05:14 [loggers.py:236] Engine 000: Avg prompt throughput: 2.1 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 79.4%\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"POST /invocations HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"POST /invocations HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:37650 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:37650 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO 12-11 16:05:24 [loggers.py:236] Engine 000: Avg prompt throughput: 25.2 tokens/s, Avg generation throughput: 16.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 79.0%\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"POST /invocations HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO 12-11 16:05:34 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 79.0%\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO:     169.254.178.2:40944 - \"GET /ping HTTP/1.1\" 200 OK\n",
    "#033[1;36m(APIServer pid=1)#033[0;0m INFO 12-11 16:05:44 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 79.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb28c5-d39e-458c-964d-88683fd6392f",
   "metadata": {},
   "source": [
    "## 6. Close the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc42b7-b33c-4b15-86c2-c7bf1bd743c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'f39176fa-4057-45c7-b215-521b79b45260', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f39176fa-4057-45c7-b215-521b79b45260', 'x-amzn-sagemaker-closed-session-id': '99680e4c-5d0e-49ac-88e7-103866569520', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Fri, 12 Dec 2025 00:17:01 GMT', 'content-type': 'application/json', 'content-length': '51', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'ClosedSessionId': '99680e4c-5d0e-49ac-88e7-103866569520', 'Body': <botocore.response.StreamingBody object at 0x7f66236b6da0>}\n",
      "âœ“ Session closed\n"
     ]
    }
   ],
   "source": [
    "# Close the session when done to free resources\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    SessionId=session_id,\n",
    "    Body=json.dumps({\"requestType\": \"CLOSE\"})\n",
    ")\n",
    "print(response)\n",
    "print(\"âœ“ Session closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fac36-ce5d-44cc-821c-40da3db8f703",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37805c8f-fc40-41fc-bc3d-2e89492ef9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Resources cleaned up\n"
     ]
    }
   ],
   "source": [
    "# Delete endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sagemaker_client.delete_model(ModelName=model_name)\n",
    "print(\"âœ“ Resources cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
