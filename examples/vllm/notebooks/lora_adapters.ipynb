{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Adapter Management with vLLM on SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy a vLLM model with LoRA adapters on Amazon SageMaker using inference components.\n",
    "\n",
    "## Overview\n",
    "\n",
    "LoRA (Low-Rank Adaptation) enables efficient fine-tuning by training small adapter weights instead of the full model. With SageMaker inference components, you can:\n",
    "\n",
    "1. Deploy a base model as a **base inference component**\n",
    "2. Deploy LoRA adapters as **child inference components** that reference the base\n",
    "3. Route requests to specific adapters by specifying the adapter's inference component name\n",
    "\n",
    "## Prerequisites\n",
    "- SageMaker execution role with appropriate permissions\n",
    "- vLLM container image in ECR\n",
    "- HuggingFace token for gated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "SageMaker Role: arn:aws:iam::875423407011:role/AdminRole\n",
      "S3 Bucket: sagemaker-us-west-2-875423407011\n",
      "Region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Name: vllm-lora-2025-12-15-23-18-42-720\n",
      "Instance Type: ml.g6e.12xlarge\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "inference_image = f'{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:0.11.2-sagemaker-v1.2'\n",
    "instance_type = \"ml.g6e.12xlarge\"\n",
    "num_gpus = 4\n",
    "\n",
    "endpoint_config_name = endpoint_name = sagemaker.utils.name_from_base(\"vllm-lora\")\n",
    "variant_name = \"main\"\n",
    "timeout = 600\n",
    "\n",
    "# Replace with your HuggingFace token\n",
    "huggingface_token = 'hf_your_token_here'\n",
    "\n",
    "print(f\"Endpoint Name: {endpoint_name}\")\n",
    "print(f\"Instance Type: {instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Base Model and LoRA Adapters\n",
    "\n",
    "Download the base model and LoRA adapters from HuggingFace, then upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5344103faa4a406192e81fb527c3ddca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdf6dc37eb84bed84ccb40bd6ffbbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded base model to: ./models/meta-llama-Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_id_pathsafe = model_id.replace(\"/\", \"-\")\n",
    "local_model_path = f\"./models/{model_id_pathsafe}\"\n",
    "s3_model_path = f\"s3://{bucket}/models/{model_id_pathsafe}\"\n",
    "\n",
    "# Download base model\n",
    "snapshot_download(\n",
    "    repo_id=model_id,\n",
    "    local_dir=local_model_path,\n",
    "    token=huggingface_token,\n",
    "    allow_patterns=[\"*.json\", \"*.safetensors\"]\n",
    ")\n",
    "\n",
    "print(f\"Downloaded base model to: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10819a2d57f4034b95b746a93235570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cd73b7714b47abb0b2a8d86e500307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d4d7d8e63041d9a8d86b08fab665ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a43f396cba46539c5bef3dffe9ec0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded adapters:\n",
      "['chinese', 'korean']\n"
     ]
    }
   ],
   "source": [
    "# Download LoRA adapters\n",
    "os.makedirs(f\"{local_model_path}/adapters\", exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"faridlazuarda/valadapt-llama-3.1-8B-it-korean\",\n",
    "    local_dir=f\"{local_model_path}/adapters/korean\"\n",
    ")\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"faridlazuarda/valadapt-llama-3.1-8B-it-chinese\",\n",
    "    local_dir=f\"{local_model_path}/adapters/chinese\"\n",
    ")\n",
    "\n",
    "print(\"Downloaded adapters:\")\n",
    "print(os.listdir(f\"{local_model_path}/adapters\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/config.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/config.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00001-of-00004.safetensors.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00001-of-00004.safetensors.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/generation_config.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/generation_config.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/config.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/config.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/.gitignore to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/.gitignore\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00002-of-00004.safetensors.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00002-of-00004.safetensors.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/generation_config.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/generation_config.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00001-of-00004.safetensors.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00001-of-00004.safetensors.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00003-of-00004.safetensors.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00003-of-00004.safetensors.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00003-of-00004.safetensors.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00003-of-00004.safetensors.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00004-of-00004.safetensors.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00004-of-00004.safetensors.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00002-of-00004.safetensors.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00002-of-00004.safetensors.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00004-of-00004.safetensors.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model-00004-of-00004.safetensors.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/original/params.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/original/params.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model.safetensors.index.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model.safetensors.index.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/original/params.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/original/params.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model.safetensors.index.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/model.safetensors.index.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/special_tokens_map.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/special_tokens_map.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/special_tokens_map.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer_config.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer_config.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/.gitignore to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/.gitignore\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer_config.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/README.md.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/README.md.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/README.md.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/README.md.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/.gitattributes.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/.gitattributes.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_config.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_config.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_config.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_config.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/.gitattributes.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/.gitattributes.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/optimizer.pt.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/optimizer.pt.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_model.safetensors.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_model.safetensors.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_model.safetensors.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/adapter_model.safetensors.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/rng_state.pth.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/rng_state.pth.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/rng_state.pth.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/rng_state.pth.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/optimizer.pt.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/optimizer.pt.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/scheduler.pt.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/scheduler.pt.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/special_tokens_map.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/special_tokens_map.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/scheduler.pt.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/scheduler.pt.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/special_tokens_map.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer_config.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer_config.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer_config.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/trainer_state.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/trainer_state.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/trainer_state.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/trainer_state.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/training_args.bin.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/training_args.bin.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/training_args.bin.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.cache/huggingface/download/training_args.bin.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/README.md to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/README.md\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/adapter_config.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/adapter_config.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.gitattributes to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/.gitattributes\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/adapter_model.safetensors to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/adapter_model.safetensors\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/rng_state.pth to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/rng_state.pth\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/scheduler.pt to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/scheduler.pt\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/special_tokens_map.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/special_tokens_map.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/tokenizer_config.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/tokenizer_config.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/trainer_state.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/trainer_state.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/training_args.bin to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/training_args.bin\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/.gitignore to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/.gitignore\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/.gitattributes.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/.gitattributes.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/README.md.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/README.md.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/.gitattributes.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/.gitattributes.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/README.md.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/README.md.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_config.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_config.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_config.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_config.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_model.safetensors.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_model.safetensors.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_model.safetensors.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/adapter_model.safetensors.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/optimizer.pt.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/optimizer.pt.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/optimizer.pt.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/optimizer.pt.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/rng_state.pth.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/rng_state.pth.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/scheduler.pt.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/scheduler.pt.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/scheduler.pt.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/scheduler.pt.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/rng_state.pth.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/rng_state.pth.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/special_tokens_map.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/special_tokens_map.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/special_tokens_map.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer_config.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/trainer_state.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/trainer_state.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer_config.json.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/tokenizer_config.json.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/training_args.bin.lock to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/training_args.bin.lock\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/training_args.bin.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/training_args.bin.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/trainer_state.json.metadata to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.cache/huggingface/download/trainer_state.json.metadata\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.gitattributes to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/.gitattributes\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/README.md to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/README.md\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/adapter_config.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/adapter_config.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/tokenizer.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/tokenizer.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/optimizer.pt to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/chinese/optimizer.pt\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/rng_state.pth to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/rng_state.pth\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/scheduler.pt to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/scheduler.pt\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/special_tokens_map.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/special_tokens_map.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/tokenizer_config.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/tokenizer_config.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/trainer_state.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/trainer_state.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/training_args.bin to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/training_args.bin\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/config.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/config.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/generation_config.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/generation_config.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/adapter_model.safetensors to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/adapter_model.safetensors\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/tokenizer.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/tokenizer.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/optimizer.pt to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/adapters/korean/optimizer.pt\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/model.safetensors.index.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/model.safetensors.index.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/original/params.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/original/params.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/special_tokens_map.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/special_tokens_map.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/tokenizer.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/tokenizer.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/tokenizer_config.json to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/tokenizer_config.json\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/model-00004-of-00004.safetensors to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/model-00004-of-00004.safetensors\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/model-00001-of-00004.safetensors to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/model-00001-of-00004.safetensors\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/model-00003-of-00004.safetensors to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/model-00003-of-00004.safetensors\n",
      "upload: models/meta-llama-Llama-3.1-8B-Instruct/model-00002-of-00004.safetensors to s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct/model-00002-of-00004.safetensors\n",
      "Uploaded to: s3://sagemaker-us-west-2-875423407011/models/meta-llama-Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Upload base model (with adapters) to S3\n",
    "!aws s3 cp --recursive {local_model_path} {s3_model_path}\n",
    "\n",
    "print(f\"Uploaded to: {s3_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Endpoint\n",
    "\n",
    "Create an endpoint configuration and endpoint. The endpoint will host inference components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created endpoint config: vllm-lora-2025-12-15-23-18-42-720\n"
     ]
    }
   ],
   "source": [
    "# Create endpoint configuration\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": timeout,\n",
    "            \"RoutingConfig\": {\n",
    "                \"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"\n",
    "            },\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": 1,\n",
    "                \"MaxInstanceCount\": 1\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Created endpoint config: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint: vllm-lora-2025-12-15-23-18-42-720\n",
      "This will take a few minutes...\n",
      "-----!Endpoint is ready!\n"
     ]
    }
   ],
   "source": [
    "# Create endpoint\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "print(\"This will take a few minutes...\")\n",
    "\n",
    "sess.wait_for_endpoint(endpoint_name)\n",
    "print(\"Endpoint is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Base Model Inference Component\n",
    "\n",
    "Create the base model as an inference component with LoRA support enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Name: llama-3-1-8b-251215-2331\n",
      "Base IC Name: ic-llama-3-1-8b-251215-2331\n"
     ]
    }
   ],
   "source": [
    "# Environment variables for vLLM with LoRA support\n",
    "env = {\n",
    "    \"SM_VLLM_MODEL\": \"/opt/ml/model\",\n",
    "    \"HF_TOKEN\": huggingface_token,\n",
    "    \"SM_VLLM_TENSOR_PARALLEL_SIZE\": \"2\",\n",
    "    \"SM_VLLM_MAX_MODEL_LEN\": \"4096\",\n",
    "    # LoRA configuration\n",
    "    \"SM_VLLM_ENABLE_LORA\": \"true\",\n",
    "    \"SM_VLLM_MAX_LORA_RANK\": \"64\",\n",
    "    \"SM_VLLM_MAX_CPU_LORAS\": \"4\",\n",
    "    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\": \"True\"\n",
    "}\n",
    "\n",
    "base_model_name = sagemaker.utils.name_from_base(\"llama-3-1-8b\", short=True)\n",
    "base_ic_name = f\"ic-{base_model_name}\"\n",
    "\n",
    "print(f\"Base Model Name: {base_model_name}\")\n",
    "print(f\"Base IC Name: {base_ic_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model: llama-3-1-8b-251215-2331\n"
     ]
    }
   ],
   "source": [
    "# Create SageMaker model\n",
    "model_response = sm_client.create_model(\n",
    "    ModelName=base_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image,\n",
    "        \"Environment\": env,\n",
    "        \"ModelDataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3Uri\": s3_model_path + \"/\",\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Created model: {base_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating base inference component: ic-llama-3-1-8b-251215-2331\n",
      "-------------!Base inference component is ready!\n"
     ]
    }
   ],
   "source": [
    "# Create base inference component\n",
    "ic_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName=base_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": base_model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": timeout,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": timeout,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": 4096,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 2,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": 1,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Creating base inference component: {base_ic_name}\")\n",
    "sess.wait_for_inference_component(base_ic_name)\n",
    "print(\"Base inference component is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Base Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Response:\n",
      "?\n",
      "Amazon has a large and diverse team of software development engineers (SDEs), and the \"best\" SDE can depend on various factors such as the specific team, project, or area of expertise. However, I can provide some general information about Amazon's SDEs and highlight a few notable individuals who have made significant contributions to the company.\n",
      "\n",
      "Amazon's SDEs are known for their expertise in various areas, including:\n",
      "\n",
      "1.  **Cloud Computing**: Amazon Web Services (AWS)\n"
     ]
    }
   ],
   "source": [
    "# Test base model (no adapter)\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=base_ic_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\n",
    "        \"prompt\": [\"hello, who is the best SDE in Amazon\"],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.0,\n",
    "    })\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read())\n",
    "print(\"Base Model Response:\")\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create LoRA Adapter Inference Component\n",
    "\n",
    "Create a child inference component for the Chinese LoRA adapter. The adapter references the base inference component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created adapter.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Package the adapter as tar.gz\n",
    "adapter_name = \"chinese\"\n",
    "adapter_local_path = f\"{local_model_path}/adapters/{adapter_name}\"\n",
    "\n",
    "with tarfile.open(\"adapter.tar.gz\", \"w:gz\") as tar:\n",
    "    for name in os.listdir(adapter_local_path):\n",
    "        fullpath = os.path.join(adapter_local_path, name)\n",
    "        tar.add(fullpath, arcname=name)\n",
    "\n",
    "print(\"Created adapter.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded adapter to: s3://sagemaker-us-west-2-875423407011/lora/chinese/adapter.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload adapter to S3\n",
    "adapter_s3_key = f\"lora/{adapter_name}/adapter.tar.gz\"\n",
    "adapter_s3_uri = f\"s3://{bucket}/{adapter_s3_key}\"\n",
    "\n",
    "s3_client.upload_file(\"adapter.tar.gz\", bucket, adapter_s3_key)\n",
    "print(f\"Uploaded adapter to: {adapter_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating adapter inference component: adapter-chinese-1\n"
     ]
    }
   ],
   "source": [
    "# Create adapter inference component (child of base IC)\n",
    "adapter_ic_name = f\"adapter-{adapter_name}-1\"\n",
    "\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=adapter_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    Specification={\n",
    "        # Reference the base inference component\n",
    "        \"BaseInferenceComponentName\": base_ic_name,\n",
    "        \"Container\": {\n",
    "            # S3 path to the adapter tar.gz\n",
    "            \"ArtifactUrl\": adapter_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Creating adapter inference component: {adapter_ic_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Chinese Adapter Inference\n",
    "\n",
    "Invoke the Chinese adapter by specifying its inference component name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vllm-lora-2025-12-15-23-18-42-720'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adapter-chinese-1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_ic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Adapter Response:\n",
      "？我想知道你对我说英语的能力有多高。 You like to listen to me speak English, don't you? How often, in your opinion, do people from India speak correctly in the movies? Would you like to see more movies featuring Indian speakers? Would you like to see more movies featuring Indian speakers? Would you like to see more movies featuring speakers of other languages? Would you like to see more movies featuring speakers of other languages? Would you like to see more movies\n"
     ]
    }
   ],
   "source": [
    "# Test with Chinese adapter\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=adapter_ic_name,  # Use adapter IC name\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\n",
    "        \"prompt\": [\"你好 你知道我说什么吗\"],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.0,\n",
    "    })\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read())\n",
    "print(\"Chinese Adapter Response:\")\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create and Test Korean Adapter\n",
    "\n",
    "Create another adapter inference component for Korean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created korean_adapter.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Package Korean adapter as tar.gz\n",
    "korean_adapter_name = \"korean\"\n",
    "korean_adapter_local_path = f\"{local_model_path}/adapters/{korean_adapter_name}\"\n",
    "\n",
    "with tarfile.open(\"korean_adapter.tar.gz\", \"w:gz\") as tar:\n",
    "    for name in os.listdir(korean_adapter_local_path):\n",
    "        fullpath = os.path.join(korean_adapter_local_path, name)\n",
    "        tar.add(fullpath, arcname=name)\n",
    "\n",
    "print(\"Created korean_adapter.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded Korean adapter to: s3://sagemaker-us-west-2-875423407011/lora/korean/adapter.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload Korean adapter to S3\n",
    "korean_s3_key = f\"lora/{korean_adapter_name}/adapter.tar.gz\"\n",
    "korean_s3_uri = f\"s3://{bucket}/{korean_s3_key}\"\n",
    "\n",
    "s3_client.upload_file(\"korean_adapter.tar.gz\", bucket, korean_s3_key)\n",
    "print(f\"Uploaded Korean adapter to: {korean_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Korean adapter inference component: adapter-korean\n",
      "!Korean adapter inference component is ready!\n"
     ]
    }
   ],
   "source": [
    "# Create Korean adapter inference component\n",
    "korean_ic_name = f\"adapter-{korean_adapter_name}\"\n",
    "\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=korean_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    Specification={\n",
    "        \"BaseInferenceComponentName\": base_ic_name,\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": korean_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Creating Korean adapter inference component: {korean_ic_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean Adapter Response:\n",
      " It's nice to see you. How's the weather today?\n",
      "What's your opinion on the frequency of political interference in the elections of this country? What's your opinion on the frequency of political interference in the elections of this country?\n",
      "Do you agree that the government of this country should have the right to keep track of all emails and other types of information shared over the internet? Do you agree that the government of this country should have the right to keep track of all emails and other types of information\n"
     ]
    }
   ],
   "source": [
    "# Test with Korean adapter\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=korean_ic_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\n",
    "        \"prompt\": [\"안녕하세요, 오늘 날씨가 어때요?\"],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.0,\n",
    "    })\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read())\n",
    "print(\"Korean Adapter Response:\")\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "Delete resources in reverse order: adapter ICs → base IC → endpoint → endpoint config → model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Chinese adapter IC: adapter-chinese-1\n",
      "Deleted Korean adapter IC: adapter-korean\n"
     ]
    }
   ],
   "source": [
    "# Delete adapter inference components\n",
    "sm_client.delete_inference_component(InferenceComponentName=adapter_ic_name)\n",
    "print(f\"Deleted Chinese adapter IC: {adapter_ic_name}\")\n",
    "\n",
    "sm_client.delete_inference_component(InferenceComponentName=korean_ic_name)\n",
    "print(f\"Deleted Korean adapter IC: {korean_ic_name}\")\n",
    "\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete base inference component\n",
    "sm_client.delete_inference_component(InferenceComponentName=base_ic_name)\n",
    "print(f\"Deleted base IC: {base_ic_name}\")\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Deleted endpoint: {endpoint_name}\")\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "print(f\"Deleted endpoint config: {endpoint_config_name}\")\n",
    "\n",
    "# Delete model\n",
    "sm_client.delete_model(ModelName=base_model_name)\n",
    "print(f\"Deleted model: {base_model_name}\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch",
   "language": "python",
   "name": "conda_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
