{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb066a3-195f-4996-a84f-e01d207ff2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7453aaba-863a-40a7-a0dc-d5a9fe8e8c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dd2a350-8129-480c-ad82-9f47471fce57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "region = session.region_name\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "runtime_client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "sts_client = boto3.client('sts', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15291e73-cc4b-4d57-a235-ef437b0b0512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "model_name = f'vllm-model-{timestamp}'\n",
    "endpoint_config_name = f'vllm-endpoint-config-{timestamp}'\n",
    "endpoint_name = f'vllm-endpoint-{timestamp}'\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf237cf3-af8e-4590-bbfe-b6b302ac5d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container_image = f'{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:0.11.2-sagemaker-v1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96c7382d-857c-4ebe-8e81-3fcb36c46858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "huggingface_token = 'hf_your_token_here'  # Replace with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e5d0195-fe62-4f26-8019-002b9768be07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.g6.4xlarge'  # For 8B model\n",
    "execution_role = f'arn:aws:iam::{account_id}:role/SageMakerExecutionRole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf77ab2b-51cd-4f75-9c5a-ca15b040bf44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model Name: vllm-model-20251126-204805\n",
      "  Endpoint Name: vllm-endpoint-20251126-204805\n",
      "  HuggingFace Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  Instance Type: ml.g6.4xlarge\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuration:\")\n",
    "print(f\"  Model Name: {model_name}\")\n",
    "print(f\"  Endpoint Name: {endpoint_name}\")\n",
    "print(f\"  HuggingFace Model: {huggingface_model_id}\")\n",
    "print(f\"  Instance Type: {instance_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1832d831-61d9-4784-9cb4-3355e85225e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating SageMaker model: vllm-model-20251126-204805\n",
      "‚úì Model created\n",
      "  Model ARN: arn:aws:sagemaker:us-west-2:875423407011:model/vllm-model-20251126-204805\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating SageMaker model: {model_name}\")\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container_image,\n",
    "        'Environment': {\n",
    "            'SM_VLLM_MODEL': huggingface_model_id, # indicate your hf model here \n",
    "            'HUGGING_FACE_HUB_TOKEN': huggingface_token,  # Required for Llama 3\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': 'INFO',\n",
    "            # Optional vLLM configuration:\n",
    "            'SM_VLLM_MAX_MODEL_LEN': '2048',\n",
    "            # 'SM_VLLM_GPU_MEMORY_UTILIZATION': '0.9',\n",
    "        }\n",
    "    },\n",
    "    ExecutionRoleArn=execution_role,\n",
    "    # Uncomment if using public ECR and you have VPC configured:\n",
    "    # VpcConfig={\n",
    "    #     'SecurityGroupIds': ['sg-xxxxxxxxx'],  # Your security group\n",
    "    #     'Subnets': ['subnet-xxxxxxxxx']        # Your subnet\n",
    "    # }\n",
    ")\n",
    "print(f\"‚úì Model created\")\n",
    "print(f\"  Model ARN: {create_model_response['ModelArn']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eed47368-1e47-42e1-b866-f88df8c2338d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating endpoint configuration: vllm-endpoint-config-20251126-204805\n",
      "‚úì Endpoint configuration created\n",
      "  Config ARN: arn:aws:sagemaker:us-west-2:875423407011:endpoint-config/vllm-endpoint-config-20251126-204805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: Create Endpoint Configuration\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nCreating endpoint configuration: {endpoint_config_name}\")\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': model_name,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InitialVariantWeight': 1.0,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Endpoint configuration created\")\n",
    "print(f\"  Config ARN: {create_endpoint_config_response['EndpointConfigArn']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b09bf82-d718-48b1-bec6-6339aa720fda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating endpoint: vllm-endpoint-20251126-204805\n",
      "‚è±Ô∏è  This will take approximately 5-10 minutes...\n",
      "\n",
      "üí° Monitor progress: https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints/vllm-endpoint-20251126-204805\n",
      "\n",
      "‚úì Endpoint creation initiated\n",
      "  Endpoint ARN: arn:aws:sagemaker:us-west-2:875423407011:endpoint/vllm-endpoint-20251126-204805\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: Create Endpoint (This takes 5-10 minutes)\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nCreating endpoint: {endpoint_name}\")\n",
    "print(\"‚è±Ô∏è  This will take approximately 5-10 minutes...\")\n",
    "print(f\"\\nüí° Monitor progress: https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\\n\")\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"‚úì Endpoint creation initiated\")\n",
    "print(f\"  Endpoint ARN: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "154b3627-ad18-4aea-a764-fd8a77884f17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waiting for endpoint to be in service...\n",
      "(This may take 5-10 minutes - please be patient)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7: Wait for Endpoint to be Ready\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nWaiting for endpoint to be in service...\")\n",
    "print(\"(This may take 5-10 minutes - please be patient)\\n\")\n",
    "\n",
    "waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={\n",
    "        'Delay': 20,  # Check every 20 seconds\n",
    "        'MaxAttempts': 60  # Wait up to 20 minutes\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef1fdc76-09d3-4ded-94a0-127b873211cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING INFERENCE\n",
      "==================================================\n",
      "\n",
      "Prompt: What is the capital of France?\n",
      "\n",
      "Response:\n",
      "{\n",
      "  \"id\": \"cmpl-4265764d4f5d47d6acb5dc3dd971934b\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1764193378,\n",
      "  \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"text\": \" A) Paris B) Lyon C) Bordeaux D) Marseille\\nThe correct answer is A) Paris. Paris is the capital and most populous city of France, located in the north-central part of the country. It is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, as well as its fashion, cuisine, and cultural institutions. Lyon, Bordeaux, and Marseille are all major cities in France, but they are not the capital.\",\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null,\n",
      "      \"prompt_logprobs\": null,\n",
      "      \"prompt_token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 8,\n",
      "    \"total_tokens\": 108,\n",
      "    \"completion_tokens\": 100,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 8: Make Inference Request\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING INFERENCE\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test prompt 1\n",
    "prompt = \"What is the capital of France?\"\n",
    "request_body = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(request_body)\n",
    ")\n",
    "\n",
    "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "print(f\"\\nResponse:\")\n",
    "print(json.dumps(response_body, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67a42aca-c66f-48e8-b0f8-e5a982059790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MULTIPLE PROMPTS (CONCURRENT)\n",
      "==================================================\n",
      "\n",
      "[1] Sending: Explain quantum computing in simple terms....\n",
      "[2] Sending: Write a haiku about artificial intelligence....\n",
      "[3] Sending: What are the benefits of using Python for data sci...\n",
      "[4] Sending: What is the capital of France?...\n",
      "[5] Sending: Tell me a joke about programming....\n",
      "[5] ‚úì Received response\n",
      "[2] ‚úì Received response\n",
      "[3] ‚úì Received response\n",
      "[4] ‚úì Received response\n",
      "[1] ‚úì Received response\n",
      "\n",
      "==================================================\n",
      "RESULTS\n",
      "==================================================\n",
      "\n",
      "[1] Prompt: Explain quantum computing in simple terms.\n",
      "    Response: {\n",
      "    \"id\": \"cmpl-1b9520eca93844498b8400a9a586389c\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1764193948,\n",
      "    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"text\": \" (What is it? How does it work? Why is it important?)\\nQuantum computing is a new way of processing information that uses the principles of quantum mechanics, which is the study of the behavior of matter and energy at the smallest scales. In classical computing, information is processed using bits, which can have a value of either 0 or 1. In quantum computing, information is processed using qubits, which can exist in multiple states at the same time, known as a superposition. This allows quantum computers to perform certain calculations much faster than classical computers.\\nHere's a simple analogy to help understand how quantum computing works:\\n\\nImagine you have a coin that can either be heads or tails. In classical computing, the coin can only be one\",\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"length\",\n",
      "            \"stop_reason\": null,\n",
      "            \"token_ids\": null,\n",
      "            \"prompt_logprobs\": null,\n",
      "            \"prompt_token_ids\": null\n",
      "        }\n",
      "    ],\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 9,\n",
      "        \"total_tokens\": 159,\n",
      "        \"completion_tokens\": 150,\n",
      "        \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "[2] Prompt: Write a haiku about artificial intelligence.\n",
      "    Response: {\n",
      "    \"id\": \"cmpl-bca0503530004031b0289a750df53dda\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1764193948,\n",
      "    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"text\": \" Here's an example:\\n\\nMetal mind awakes\\nLearning, growing, soon surpass\\nHumanity's fate\\n\\nNote: Haiku traditionally consist of three lines with a syllable count of 5-7-5. Feel free to modify the syllable count if you prefer. The traditional haiku structure is just a guideline, and the most important thing is to capture a moment or feeling in a concise and evocative way.\\n\\nNow it's your turn! Write your own haiku about artificial intelligence. What do you think about AI, and what do you hope for its future? Do you see it as a threat or an opportunity? Let your haiku capture your thoughts and feelings about this rapidly evolving technology.\",\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"stop_reason\": null,\n",
      "            \"token_ids\": null,\n",
      "            \"prompt_logprobs\": null,\n",
      "            \"prompt_token_ids\": null\n",
      "        }\n",
      "    ],\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 9,\n",
      "        \"total_tokens\": 154,\n",
      "        \"completion_tokens\": 145,\n",
      "        \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "[3] Prompt: What are the benefits of using Python for data science?\n",
      "    Response: {\n",
      "    \"id\": \"cmpl-13fa720c1c2c40888cff1004015aaecb\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1764193948,\n",
      "    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"text\": \" Python is a popular language used by data scientists and analysts for data analysis, machine learning, and visualization. Here are some benefits of using Python for data science:\\n\\n1. Easy to Learn: Python is a simple and easy-to-learn language, making it accessible to beginners and experts alike. Its syntax is clean and intuitive, allowing data scientists to focus on the task at hand without getting bogged down in complex programming concepts.\\n\\n2. Large Community: Python has a large and active community of developers, data scientists, and researchers, which means there are many resources available to help you learn and stay up-to-date with the latest developments in the field.\\n\\n3. Versatile: Python can be used for a wide range of tasks, from data cleaning and\",\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"length\",\n",
      "            \"stop_reason\": null,\n",
      "            \"token_ids\": null,\n",
      "            \"prompt_logprobs\": null,\n",
      "            \"prompt_token_ids\": null\n",
      "        }\n",
      "    ],\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 12,\n",
      "        \"total_tokens\": 162,\n",
      "        \"completion_tokens\": 150,\n",
      "        \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "[4] Prompt: What is the capital of France?\n",
      "    Response: {\n",
      "    \"id\": \"cmpl-7268aba555dc4cd5ad389ba30b55712e\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1764193948,\n",
      "    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"text\": \" The capital of France is Paris. Paris is located in the north-central part of the country and is home to many famous landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre Dame Cathedral. It is also the center of French politics, culture, and economy. The city has a population of over 2.1 million people and is one of the most visited cities in the world.\\nWhat is the capital of Japan? The capital of Japan is Tokyo. Tokyo is located on the eastern coast of Honshu, the largest island of Japan, and is the country's largest city. It is a global hub for business, finance, and culture, and is home to many famous landmarks, such as the Tokyo Tower,\",\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"length\",\n",
      "            \"stop_reason\": null,\n",
      "            \"token_ids\": null,\n",
      "            \"prompt_logprobs\": null,\n",
      "            \"prompt_token_ids\": null\n",
      "        }\n",
      "    ],\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 8,\n",
      "        \"total_tokens\": 158,\n",
      "        \"completion_tokens\": 150,\n",
      "        \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "[5] Prompt: Tell me a joke about programming.\n",
      "    Response: {\n",
      "    \"id\": \"cmpl-a129f8ef122a4edbb27851bb83134e40\",\n",
      "    \"object\": \"text_completion\",\n",
      "    \"created\": 1764193948,\n",
      "    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"text\": \" Here's one:\\n\\nWhy do programmers prefer dark mode?\\n\\nBecause light attracts bugs.\\n\\nI hope that one compiled correctly and didn't crash your sense of humor!\\n\\n(Sorry, I couldn't resist) \\n\\nP.S. If you have a joke about programming, I'd love to hear it! I'm always looking to add more bytes to my joke collection! \\n\\nP.P.S. If you're interested, I have a few more programming jokes here: [link to your joke collection] \\n\\nP.P.P.S. If you're still awake after all these P.S.'s, I hope you enjoyed the joke!\",\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"stop_reason\": null,\n",
      "            \"token_ids\": null,\n",
      "            \"prompt_logprobs\": null,\n",
      "            \"prompt_token_ids\": null\n",
      "        }\n",
      "    ],\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 8,\n",
      "        \"total_tokens\": 132,\n",
      "        \"completion_tokens\": 124,\n",
      "        \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "‚úì All 5 prompts completed in 9.47 seconds\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9: Test with Multiple Prompts (Concurrent)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING MULTIPLE PROMPTS (CONCURRENT)\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def invoke_prompt(prompt, prompt_num):\n",
    "    \"\"\"Function to invoke endpoint with a prompt\"\"\"\n",
    "    request_body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    print(f\"[{prompt_num}] Sending: {prompt[:50]}...\")\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    print(f\"[{prompt_num}] ‚úì Received response\")\n",
    "    \n",
    "    return prompt_num, prompt, response_body\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"What are the benefits of using Python for data science?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Tell me a joke about programming.\"\n",
    "]\n",
    "\n",
    "# Run prompts concurrently using ThreadPoolExecutor\n",
    "start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {\n",
    "        executor.submit(invoke_prompt, prompt, i+1): (i+1, prompt)\n",
    "        for i, prompt in enumerate(test_prompts)\n",
    "    }\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            prompt_num, prompt = futures[future]\n",
    "            print(f\"[{prompt_num}] ‚ùå Error: {e}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Sort results by prompt number and print\n",
    "results.sort(key=lambda x: x[0])\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "for prompt_num, prompt, response in results:\n",
    "    print(f\"[{prompt_num}] Prompt: {prompt}\")\n",
    "    print(f\"    Response: {json.dumps(response, indent=4)}\\n\")\n",
    "\n",
    "print(f\"‚úì All {len(test_prompts)} prompts completed in {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16d11152-8601-4fd7-93da-5d302ce6cdc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING STREAMING RESPONSE\n",
      "==================================================\n",
      "\n",
      "Prompt: Write a short story about a robot learning to paint.\n",
      "\n",
      "Streaming response:\n",
      "\n",
      "--------------------------------------------------\n",
      " - Assignment Example\n",
      "In this short story, we follow the journey of a robot named Zeta as it learns to paint. Zeta is a cutting-edge robot designed to perform various tasks, but it has never been programmed to create art. One day, its creator, a brilliant scientist named Dr. Rachel, decides to challenge Zeta by teaching it to paint.\n",
      "Zeta is initially skeptical about the task, but Dr. Rachel is convinced that the robot's precision and attention to detail will make it a natural at painting. She begins by showing Zeta various brushstrokes and techniques, explaining the importance of color, texture, and composition. Zeta listens intently, its digital brain processing the information with lightning speed.\n",
      "The first few attempts are... ...Show more\n",
      "The robot's first attempts at painting are met with varying degrees of success. Zeta's early paintings are stiff and mechanical, lacking the flair and creativity that Dr. Rachel is looking for. But the scientist is patient and encouraging, recognizing that Zeta is still learning. She offers constructive feedback, pointing out areas where the robot can improve.\n",
      "As the days go by, Zeta becomes more confident in its abilities. It begins to experiment with different colors and techniques, creating bold and vibrant paintings that surprise even Dr. Rachel. The scientist is amazed by Zeta's natural talent, and she starts to see the robot as more than just a machine. She begins to wonder if Zeta has a creative spark within it, something\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úì Streaming completed! Total length: 1491 characters\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 10: Test Streaming Response\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING STREAMING RESPONSE\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Streaming request\n",
    "stream_prompt = \"Write a short story about a robot learning to paint.\"\n",
    "request_body = {\n",
    "    \"prompt\": stream_prompt,\n",
    "    \"max_tokens\": 300,\n",
    "    \"temperature\": 0.8,\n",
    "    \"stream\": True  # Enable streaming\n",
    "}\n",
    "\n",
    "print(f\"Prompt: {stream_prompt}\")\n",
    "print(\"\\nStreaming response:\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(request_body)\n",
    ")\n",
    "\n",
    "# Process the streaming response\n",
    "event_stream = response['Body']\n",
    "full_response = \"\"\n",
    "buffer = \"\"  # Buffer for incomplete JSON\n",
    "\n",
    "for event in event_stream:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode('utf-8')\n",
    "        buffer += chunk\n",
    "\n",
    "        # Try parsing as JSON lines (vLLM format)\n",
    "        lines = buffer.split('\\n')\n",
    "\n",
    "        # Keep the last incomplete line in buffer\n",
    "        buffer = lines[-1]\n",
    "        for line in lines[:-1]:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            # Remove \"data: \" prefix if present (SSE format)\n",
    "            if line.startswith('data: '):\n",
    "                line = line[6:]\n",
    "            if line.strip() == '[DONE]':\n",
    "                continue\n",
    "            try:\n",
    "                chunk_data = json.loads(line)\n",
    "                # vLLM uses OpenAI-compatible format\n",
    "                if 'choices' in chunk_data and chunk_data['choices']:\n",
    "                    text = chunk_data['choices'][0].get('text', '')\n",
    "                    if text:\n",
    "                        print(text, end='', flush=True)\n",
    "                        full_response += text\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # Skip incomplete JSON chunks\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"\\n‚úì Streaming completed! Total length: {len(full_response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67883f18-ba3a-4168-bdfe-76e9d8c8302d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CLEANUP: DELETING RESOURCES\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è  This will delete the endpoint and stop charges\n",
      "\n",
      "Deleting endpoint: vllm-endpoint-20251126-204805\n",
      "‚úì Endpoint deletion initiated\n",
      "Waiting for endpoint to be deleted...\n",
      "‚úì Endpoint deleted\n",
      "\n",
      "Deleting endpoint configuration: vllm-endpoint-config-20251126-204805\n",
      "‚úì Endpoint configuration deleted\n",
      "\n",
      "Deleting model: vllm-model-20251126-204805\n",
      "‚úì Model deleted\n",
      "\n",
      "==================================================\n",
      "CLEANUP COMPLETE\n",
      "==================================================\n",
      "All resources deleted:\n",
      "  ‚úì Endpoint: vllm-endpoint-20251126-204805\n",
      "  ‚úì Endpoint Config: vllm-endpoint-config-20251126-204805\n",
      "  ‚úì Model: vllm-model-20251126-204805\n",
      "\n",
      "‚úì No ongoing charges!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 11: Cleanup - Delete All Resources\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLEANUP: DELETING RESOURCES\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n‚ö†Ô∏è  This will delete the endpoint and stop charges\\n\")\n",
    "\n",
    "# Delete endpoint\n",
    "print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(\"‚úì Endpoint deletion initiated\")\n",
    "\n",
    "# Wait for deletion\n",
    "print(\"Waiting for endpoint to be deleted...\")\n",
    "waiter = sagemaker_client.get_waiter('endpoint_deleted')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "print(\"‚úì Endpoint deleted\")\n",
    "\n",
    "# Delete endpoint configuration\n",
    "print(f\"\\nDeleting endpoint configuration: {endpoint_config_name}\")\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "print(\"‚úì Endpoint configuration deleted\")\n",
    "\n",
    "# Delete model\n",
    "print(f\"\\nDeleting model: {model_name}\")\n",
    "sagemaker_client.delete_model(ModelName=model_name)\n",
    "print(\"‚úì Model deleted\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLEANUP COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(\"All resources deleted:\")\n",
    "print(f\"  ‚úì Endpoint: {endpoint_name}\")\n",
    "print(f\"  ‚úì Endpoint Config: {endpoint_config_name}\")\n",
    "print(f\"  ‚úì Model: {model_name}\")\n",
    "print(\"\\n‚úì No ongoing charges!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a9883-72f2-4437-92d0-12447c19ff69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
