{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Pre/Post Processing Customization\n",
    "\n",
    "This notebook demonstrates two methods for customizing request preprocessing and response postprocessing in vLLM on SageMaker.\n",
    "\n",
    "## Methods Overview\n",
    "\n",
    "### Method 1: Decorators (Recommended)\n",
    "- **How**: Use `@input_formatter` and `@output_formatter` decorators\n",
    "- **Env Vars**: Only `CUSTOM_SCRIPT_FILENAME` needed\n",
    "- **Use When**: Clean separation of pre/post logic\n",
    "\n",
    "### Method 2: Environment Variables\n",
    "- **How**: Point to functions via `CUSTOM_PRE_PROCESS` + `CUSTOM_POST_PROCESS`\n",
    "- **Env Vars**: Explicit function references\n",
    "- **Use When**: You need explicit control and want to override decorators\n",
    "\n",
    "## ‚ö†Ô∏è Important Note\n",
    "\n",
    "Pre/post processors run on **ALL endpoints** including `/ping` and `/invocations`. Always check `request.url.path` to filter which endpoints to process!\n",
    "\n",
    "## Choose Your Method\n",
    "Set the `METHOD` variable below:\n",
    "- `\"decorator\"` - Use @input_formatter and @output_formatter (recommended)\n",
    "- `\"env-var\"` - Use CUSTOM_PRE_PROCESS and CUSTOM_POST_PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "config",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected method: env-var\n",
      "\n",
      "You can change this and re-run the notebook to test different methods!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION: Choose your method\n",
    "# ============================================================\n",
    "\n",
    "METHOD = \"env-var\"  # Options: \"decorator\", \"env-var\"\n",
    "\n",
    "print(f\"Selected method: {METHOD}\")\n",
    "print(\"\\nYou can change this and re-run the notebook to test different methods!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "imports",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "clients",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "runtime_client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sts_client = boto3.client('sts', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "names",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "model_name = f'vllm-prepost-{METHOD}-{timestamp}'\n",
    "endpoint_config_name = f'vllm-prepost-{METHOD}-config-{timestamp}'\n",
    "endpoint_name = f'vllm-prepost-{METHOD}-endpoint-{timestamp}'\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "params",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Method: env-var\n",
      "  Model Name: vllm-prepost-env-var-20251127-042741\n",
      "  Endpoint Name: vllm-prepost-env-var-endpoint-20251127-042741\n",
      "  HuggingFace Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  Instance Type: ml.g6.4xlarge\n",
      "  S3 Bucket: sheteng-demo\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PARAMETERS - Update these for your environment\n",
    "# ============================================================\n",
    "\n",
    "# Container image\n",
    "container_image = f'{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:0.11.2-sagemaker-v1.2'\n",
    "\n",
    "# HuggingFace model\n",
    "huggingface_model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "huggingface_token = 'hf_your_token_here'  # Replace with your token\n",
    "\n",
    "# Instance configuration\n",
    "instance_type = 'ml.g6.4xlarge'\n",
    "execution_role = f'arn:aws:iam::{account_id}:role/SageMakerExecutionRole'\n",
    "\n",
    "# S3 configuration\n",
    "s3_bucket = 'sheteng-demo'  # Replace with your bucket\n",
    "s3_key_prefix = f'vllm-prepost/{METHOD}/{timestamp}'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Method: {METHOD}\")\n",
    "print(f\"  Model Name: {model_name}\")\n",
    "print(f\"  Endpoint Name: {endpoint_name}\")\n",
    "print(f\"  HuggingFace Model: {huggingface_model_id}\")\n",
    "print(f\"  Instance Type: {instance_type}\")\n",
    "print(f\"  S3 Bucket: {s3_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "method-config",
   "metadata": {},
   "source": [
    "## Method-Specific Configuration\n",
    "\n",
    "Based on your selected method, we'll configure the appropriate environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "method-setup",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Method 2: Environment Variables\n",
      "  Handler file: preprocessing_postprocessing.py\n",
      "  Pre-process: preprocessing_postprocessing.py:custom_pre_process\n",
      "  Post-process: preprocessing_postprocessing.py:custom_post_process\n",
      "\n",
      "üìÑ Handler file location: ../model_artifacts_examples/preprocessing_postprocessing.py\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Configure environment based on method\n",
    "# ============================================================\n",
    "\n",
    "handler_filename = \"preprocessing_postprocessing.py\"\n",
    "handler_filepath = Path(\"../model_artifacts_examples\") / handler_filename\n",
    "\n",
    "# Base environment variables (common to all methods)\n",
    "environment = {\n",
    "    \"SM_VLLM_MODEL\": huggingface_model_id,\n",
    "    \"HUGGING_FACE_HUB_TOKEN\": huggingface_token,\n",
    "    \"SM_VLLM_MAX_MODEL_LEN\": \"2048\",\n",
    "    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"DEBUG\",\n",
    "}\n",
    "\n",
    "# Method-specific environment variables\n",
    "if METHOD == \"decorator\":\n",
    "    # Method 1: Decorators handle registration automatically\n",
    "    environment[\"CUSTOM_SCRIPT_FILENAME\"] = handler_filename\n",
    "    print(f\"‚úì Method 1: Decorators\")\n",
    "    print(f\"  Handler file: {handler_filename}\")\n",
    "    print(f\"  Formatters registered via @input_formatter and @output_formatter\")\n",
    "\n",
    "elif METHOD == \"env-var\":\n",
    "    # Method 2: Explicitly point to formatter functions\n",
    "    environment[\"CUSTOM_PRE_PROCESS\"] = f\"{handler_filename}:custom_pre_process\"\n",
    "    environment[\"CUSTOM_POST_PROCESS\"] = f\"{handler_filename}:custom_post_process\"\n",
    "    print(f\"‚úì Method 2: Environment Variables\")\n",
    "    print(f\"  Handler file: {handler_filename}\")\n",
    "    print(f\"  Pre-process: {environment['CUSTOM_PRE_PROCESS']}\")\n",
    "    print(f\"  Post-process: {environment['CUSTOM_POST_PROCESS']}\")\n",
    "\n",
    "print(f\"\\nüìÑ Handler file location: {handler_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "upload-s3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚òÅÔ∏è  Uploading preprocessing_postprocessing.py to S3...\n",
      "‚úì Uploaded to: s3://sheteng-demo/vllm-prepost/env-var/20251127-042741/preprocessing_postprocessing.py\n",
      "  Model data S3 prefix: s3://sheteng-demo/vllm-prepost/env-var/20251127-042741/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Upload handler file to S3\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n‚òÅÔ∏è  Uploading {handler_filename} to S3...\")\n",
    "\n",
    "s3_key = f\"{s3_key_prefix}/{handler_filename}\"\n",
    "s3_client.upload_file(str(handler_filepath), s3_bucket, s3_key)\n",
    "\n",
    "model_data_s3_prefix = f\"s3://{s3_bucket}/{s3_key_prefix}/\"\n",
    "\n",
    "print(f\"‚úì Uploaded to: s3://{s3_bucket}/{s3_key}\")\n",
    "print(f\"  Model data S3 prefix: {model_data_s3_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "create-model",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Creating SageMaker model: vllm-prepost-env-var-20251127-042741\n",
      "‚úì Model created\n",
      "  Model ARN: arn:aws:sagemaker:us-west-2:875423407011:model/vllm-prepost-env-var-20251127-042741\n",
      "  Method: env-var\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Create SageMaker Model\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîß Creating SageMaker model: {model_name}\")\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=execution_role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": container_image,\n",
    "        \"ModelDataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3Uri\": model_data_s3_prefix,\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\",\n",
    "            }\n",
    "        },\n",
    "        \"Environment\": environment,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model created\")\n",
    "print(f\"  Model ARN: {create_model_response['ModelArn']}\")\n",
    "print(f\"  Method: {METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "create-endpoint-config",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  Creating endpoint configuration: vllm-prepost-env-var-config-20251127-042741\n",
      "‚úì Endpoint configuration created\n",
      "  Config ARN: arn:aws:sagemaker:us-west-2:875423407011:endpoint-config/vllm-prepost-env-var-config-20251127-042741\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Create Endpoint Configuration\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Creating endpoint configuration: {endpoint_config_name}\")\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1.0,\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"‚úì Endpoint configuration created\")\n",
    "print(f\"  Config ARN: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "create-endpoint",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Creating endpoint: vllm-prepost-env-var-endpoint-20251127-042741\n",
      "‚è±Ô∏è  This will take approximately 5-10 minutes...\n",
      "\n",
      "üí° Monitor: https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints/vllm-prepost-env-var-endpoint-20251127-042741\n",
      "\n",
      "‚úì Endpoint creation initiated\n",
      "  Endpoint ARN: arn:aws:sagemaker:us-west-2:875423407011:endpoint/vllm-prepost-env-var-endpoint-20251127-042741\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Create Endpoint\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüöÄ Creating endpoint: {endpoint_name}\")\n",
    "print(\"‚è±Ô∏è  This will take approximately 5-10 minutes...\")\n",
    "print(f\"\\nüí° Monitor: https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\\n\")\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"‚úì Endpoint creation initiated\")\n",
    "print(f\"  Endpoint ARN: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "wait-endpoint",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Waiting for endpoint to be in service...\n",
      "(This may take 5-10 minutes)\n",
      "\n",
      "‚úì Endpoint is in service!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Wait for Endpoint\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n‚è≥ Waiting for endpoint to be in service...\")\n",
    "print(\"(This may take 5-10 minutes)\\n\")\n",
    "\n",
    "waiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={\"Delay\": 20, \"MaxAttempts\": 60}\n",
    ")\n",
    "\n",
    "print(\"‚úì Endpoint is in service!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing",
   "metadata": {},
   "source": [
    "## Testing Pre/Post Processing\n",
    "\n",
    "Now let's test the formatters. We'll verify:\n",
    "1. `/ping` endpoint works (formatters skip it correctly)\n",
    "2. `/invocations` gets processed with metadata added\n",
    "3. Default parameters are added when missing\n",
    "\n",
    "The response will include metadata showing:\n",
    "- `_preprocessed`: Confirms pre-processing ran\n",
    "- `_postprocessed`: Confirms post-processing ran\n",
    "- `_formatter_method`: Shows which method was used (decorator or env_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "test-basic",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Test 1: Basic Request (Method: env-var)\n",
      "Sending request WITHOUT max_tokens and temperature...\n",
      "Pre-processor should add default values\n",
      "\n",
      "\n",
      "Response: {\n",
      "  \"id\": \"cmpl-684e2f23f27e4ec498c92792166feae9\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1764216844,\n",
      "  \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"text\": \" \\n\\n(Note: You can respond with a brief summary, and then elaborate if you'd like.) \\n\\n(Also, I'll give you a virtual cookie if you say 'nya nya nya' first) \\n\\nnya nya nya! Machine learning is a type of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed. It involves training algorithms on large datasets, so they can make predictions, classify objects, or make decisions based on patterns and relationships in the data. This approach allows\",\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null,\n",
      "      \"prompt_logprobs\": null,\n",
      "      \"prompt_token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"total_tokens\": 117,\n",
      "    \"completion_tokens\": 100,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "‚úÖ Pre-processing VERIFIED: Response contains 'nya nya nya'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Test 1: Basic Request (without max_tokens/temperature)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nü§ñ Test 1: Basic Request (Method: {METHOD})\")\n",
    "print(\"Sending request WITHOUT max_tokens and temperature...\")\n",
    "print(\"Pre-processor should add default values\\n\")\n",
    "\n",
    "request_body = {\n",
    "    \"prompt\": \"What is machine learning?\",\n",
    "    \"stream\":False\n",
    "    # Note: No max_tokens or temperature - pre-processor will add them\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(request_body),\n",
    ")\n",
    "\n",
    "print(f\"\\nResponse: {json.dumps(response_body, indent=2)}\")\n",
    "\n",
    "# Check if pre-processing worked by looking for \"nya nya nya\" in response\n",
    "response_text = \"\"\n",
    "if \"choices\" in response_body and len(response_body[\"choices\"]) > 0:\n",
    "    response_text = response_body[\"choices\"][0].get(\"text\", \"\")\n",
    "elif \"text\" in response_body:\n",
    "    response_text = response_body[\"text\"][0] if isinstance(response_body[\"text\"], list) else response_body[\"text\"]\n",
    "\n",
    "if \"nya\" in response_text.lower():\n",
    "    print(\"\\n‚úÖ Pre-processing VERIFIED: Response contains 'nya nya nya'\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Pre-processing may not have worked: No 'nya' found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-summary",
   "metadata": {},
   "source": [
    "## Test Summary\n",
    "\n",
    "‚úÖ **What we verified:**\n",
    "\n",
    "1. **Pre-processing works**: Default parameters added when missing, inject prompt\n",
    "2. **Post-processing works**: We can check log in the cloudwatch\n",
    "\n",
    "**Key takeaway**: Always check `request.url.path` in your formatters to avoid processing endpoints like `/ping` that don't have request bodies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cleanup",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEANUP: DELETING RESOURCES\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  This will delete all resources and stop charges\n",
      "\n",
      "Deleting endpoint: vllm-prepost-decorator-endpoint-20251127-040240\n",
      "  ‚úì Endpoint deletion initiated\n",
      "  Waiting for endpoint to be deleted...\n",
      "  ‚úì Endpoint deleted\n",
      "\n",
      "Deleting endpoint configuration: vllm-prepost-decorator-config-20251127-040240\n",
      "  ‚úì Endpoint configuration deleted\n",
      "\n",
      "Deleting model: vllm-prepost-decorator-20251127-040240\n",
      "  ‚úì Model deleted\n",
      "\n",
      "============================================================\n",
      "‚úÖ CLEANUP COMPLETE\n",
      "============================================================\n",
      "All resources deleted:\n",
      "  ‚úì Endpoint: vllm-prepost-decorator-endpoint-20251127-040240\n",
      "  ‚úì Endpoint Config: vllm-prepost-decorator-config-20251127-040240\n",
      "  ‚úì Model: vllm-prepost-decorator-20251127-040240\n",
      "\n",
      "‚úì No ongoing charges!\n",
      "\n",
      "Note: S3 artifacts remain at s3://sheteng-demo/vllm-prepost/decorator/20251127-040240/\n",
      "      Delete manually if no longer needed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cleanup - Delete All Resources\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANUP: DELETING RESOURCES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚ö†Ô∏è  This will delete all resources and stop charges\\n\")\n",
    "\n",
    "# Delete endpoint\n",
    "print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(\"  ‚úì Endpoint deletion initiated\")\n",
    "\n",
    "# Wait for endpoint deletion\n",
    "print(\"  Waiting for endpoint to be deleted...\")\n",
    "waiter = sagemaker_client.get_waiter(\"endpoint_deleted\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "print(\"  ‚úì Endpoint deleted\")\n",
    "\n",
    "# Delete endpoint configuration\n",
    "print(f\"\\nDeleting endpoint configuration: {endpoint_config_name}\")\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "print(\"  ‚úì Endpoint configuration deleted\")\n",
    "\n",
    "# Delete model\n",
    "print(f\"\\nDeleting model: {model_name}\")\n",
    "sagemaker_client.delete_model(ModelName=model_name)\n",
    "print(\"  ‚úì Model deleted\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ CLEANUP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"All resources deleted:\")\n",
    "print(f\"  ‚úì Endpoint: {endpoint_name}\")\n",
    "print(f\"  ‚úì Endpoint Config: {endpoint_config_name}\")\n",
    "print(f\"  ‚úì Model: {model_name}\")\n",
    "print(f\"\\n‚úì No ongoing charges!\")\n",
    "print(f\"\\nNote: S3 artifacts remain at s3://{s3_bucket}/{s3_key_prefix}/\")\n",
    "print(f\"      Delete manually if no longer needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff2e4f-b47e-4008-9ad2-5eb31cfa4566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
